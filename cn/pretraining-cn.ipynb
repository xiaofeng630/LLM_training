{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c8fcbe8-6c3e-473d-b25d-1f83ce8f7e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------Data Loader------ ###\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import os\n",
    "class GPTDatasetV1(Dataset): \n",
    "    def __init__(self, jsonl_path, tokenizer, max_length, stride):\n",
    "\n",
    "        self.jsonl_path = jsonl_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.line_offsets = []\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "        with open(self.jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            offset = 0\n",
    "            for line in f:\n",
    "                obj = json.loads(line)\n",
    "                tokens = tokenizer.encode(obj.get(\"text\"))\n",
    "                if(len(tokens) > self.max_length): \n",
    "                    self.line_offsets.append(offset)\n",
    "                offset += len(line.encode(\"utf-8\"))\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.line_offsets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. 定位到对应行\n",
    "        with open(self.jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            f.seek(self.line_offsets[idx])\n",
    "            line = f.readline()\n",
    "\n",
    "        # 2. 解析 json\n",
    "        obj = json.loads(line)\n",
    "        text = obj[\"text\"]\n",
    "\n",
    "        # 3. tokenize\n",
    "        token_ids = self.tokenizer.encode(text)\n",
    "\n",
    "        # 4. 如果文本太短，直接跳过（或 pad）\n",
    "        if len(token_ids) <= self.max_length + 1:\n",
    "            input_ids = token_ids[:-1]\n",
    "            target_ids = token_ids[1:]\n",
    "            # continue\n",
    "        else:\n",
    "            input_ids = token_ids[:self.max_length]\n",
    "            target_ids = token_ids[1:self.max_length + 1]\n",
    "\n",
    "        return (\n",
    "            torch.tensor(input_ids, dtype=torch.long),\n",
    "            torch.tensor(target_ids, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "class GPTDatasetV2(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, max_length, stride):\n",
    "        \"\"\"\n",
    "        data_path: str\n",
    "            - jsonl 文件路径\n",
    "            - 或包含多个 jsonl 的目录\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "\n",
    "        # 统一成文件列表\n",
    "        if os.path.isdir(data_path):\n",
    "            self.files = [\n",
    "                os.path.join(data_path, f)\n",
    "                for f in os.listdir(data_path)\n",
    "                if f.endswith(\".jsonl\")\n",
    "            ]\n",
    "        else:\n",
    "            self.files = [data_path]\n",
    "\n",
    "        # 核心索引：(file_path, byte_offset)\n",
    "        self.index = []\n",
    "\n",
    "        for file_path in self.files:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                offset = 0\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                        text = obj.get(\"text\", \"\")\n",
    "                        tokens = tokenizer.encode(text)\n",
    "                        if len(tokens) > self.max_length:\n",
    "                            self.index.append((file_path, offset))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    offset += len(line.encode(\"utf-8\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, offset = self.index[idx]\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            f.seek(offset)\n",
    "            line = f.readline()\n",
    "\n",
    "        obj = json.loads(line)\n",
    "        text = obj[\"text\"]\n",
    "\n",
    "        token_ids = self.tokenizer.encode(text)\n",
    "\n",
    "        # ====== 滑窗切分（真正利用 stride） ======\n",
    "        if len(token_ids) <= self.max_length + 1:\n",
    "            input_ids = token_ids[:-1]\n",
    "            target_ids = token_ids[1:]\n",
    "        else:\n",
    "            start = 0\n",
    "            input_ids = token_ids[start:start + self.max_length]\n",
    "            target_ids = token_ids[start + 1:start + self.max_length + 1]\n",
    "\n",
    "        return (\n",
    "            torch.tensor(input_ids, dtype=torch.long),\n",
    "            torch.tensor(target_ids, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, \n",
    "                          shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    dataset = GPTDatasetV2(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, \n",
    "                            shuffle=shuffle, drop_last=drop_last, \n",
    "                            num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c77b46c3-ad75-4b5d-90cf-1d2661137055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MultiHeadAttention(nn.Module):     \n",
    "    def __init__(self, d_in, d_out,                  \n",
    "                 context_length, dropout, num_heads, qkv_bias=False):         \n",
    "        super().__init__()         \n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out = d_out       \n",
    "        self.num_heads = num_heads \n",
    "        self.head_dim = d_out // num_heads         \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)         \n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)         \n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)         \n",
    "        self.out_proj = nn.Linear(d_out, d_out)         \n",
    "        self.dropout = nn.Dropout(dropout)        \n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)         \n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        b, num_tokens, d_in = x.shape\n",
    "        \n",
    "        # (batch, num_token, d_out) \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # (batch, num_token, num_heads, head_dim)\n",
    "        # d_out = num_heads * head_dim\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "\n",
    "        # (b, num_heads, num_token, head_dim)\n",
    "        keys = keys.transpose(1, 2) \n",
    "        queries = queries.transpose(1, 2) \n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # (b, num_heads, num_token, head_dim) @ (b, num_heads, head_dim, num_token) -> (b, num_heads, num_token, num_token)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf) \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1) \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # (b, num_token, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # (b, num_tokens, n_heads, head_dim)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b55e27-03b8-44f7-869c-e921736b1a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNorm(nn.Module):     \n",
    "    def __init__(self, emb_dim):         \n",
    "        super().__init__()         \n",
    "        self.eps = 1e-5         \n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))         \n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        \n",
    "    def forward(self, x):         \n",
    "        mean = x.mean(dim=-1, keepdim=True)         \n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)         \n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)         \n",
    "        return self.scale * norm_x + self.shift \n",
    "\n",
    "class GELU(nn.Module):     \n",
    "    def __init__(self):         \n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class FeedForward(nn.Module):     \n",
    "    def __init__(self, cfg):         \n",
    "        super().__init__()         \n",
    "        self.layers = nn.Sequential(             \n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),             \n",
    "            GELU(),             \n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),         \n",
    "        )\n",
    "\n",
    "    def forward(self, x):         \n",
    "        return self.layers(x)\n",
    "\n",
    "class TransformerBlock(nn.Module): \n",
    "    def __init__(self, cfg):         \n",
    "        super().__init__()         \n",
    "        self.att = MultiHeadAttention(\n",
    "             d_in=cfg[\"emb_dim\"],             \n",
    "             d_out=cfg[\"emb_dim\"],             \n",
    "             context_length=cfg[\"context_length\"],             \n",
    "             num_heads=cfg[\"n_heads\"],             \n",
    "             dropout=cfg[\"drop_rate\"],             \n",
    "             qkv_bias=cfg[\"qkv_bias\"]\n",
    "        ) \n",
    "        self.ff = FeedForward(cfg)         \n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])         \n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])         \n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x # prepare to use residual network        \n",
    "        x = self.norm1(x) # Normalization\n",
    "        x = self.att(x)    \n",
    "        \n",
    "        x = self.drop_shortcut(x) \n",
    "        # print(\"x.shape: \", x.shape)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x \n",
    "        x = self.norm2(x)         \n",
    "        x = self.ff(x)         \n",
    "        x = self.drop_shortcut(x)        \n",
    "        x = x + shortcut          \n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):     \n",
    "    def __init__(self, cfg):         \n",
    "        super().__init__()         \n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])         \n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])         \n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(             \n",
    "            *[TransformerBlock(cfg) \n",
    "              for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])         \n",
    "        self.out_head = nn.Linear(             \n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False         \n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):         \n",
    "        batch_size, seq_len = in_idx.shape          \n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(          \n",
    "            torch.arange(seq_len, device=in_idx.device)         \n",
    "        )         \n",
    "        x = tok_embeds + pos_embeds         \n",
    "        x = self.drop_emb(x)         \n",
    "        x = self.trf_blocks(x)         \n",
    "        x = self.final_norm(x)         \n",
    "        logits = self.out_head(x)         \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa5df6db-6a1b-4043-97e5-0d81c6503c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def generate_text_simple(model, idx,                          \n",
    "                         max_new_tokens, context_size,\n",
    "                         temperature=1.0, top_k=50, top_p=0.9, \n",
    "                         repetition_penalty=1.2):     \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        # 1. 应用重复惩罚\n",
    "        if repetition_penalty != 1.0:\n",
    "            for token_id in set(idx[0].tolist()):\n",
    "                logits[0, token_id] /= repetition_penalty\n",
    "        \n",
    "        # 2. 应用温度\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # 3. Top-k过滤\n",
    "        if top_k > 0:\n",
    "            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "            logits[indices_to_remove] = -float('Inf')\n",
    "        \n",
    "        # 4. Top-p (nucleus) 过滤\n",
    "        if top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            \n",
    "            # 移除累积概率超过top_p的token\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            \n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(\n",
    "                1, sorted_indices, sorted_indices_to_remove\n",
    "            )\n",
    "            logits[indices_to_remove] = -float('Inf')\n",
    "        \n",
    "        # 5. 采样而非贪婪选择\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probas, num_samples=1)\n",
    "        \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):     \n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})     \n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) \n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):     \n",
    "    flat = token_ids.squeeze(0)   \n",
    "    return tokenizer.decode(flat.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68282526-a4bb-4246-970c-cf943e6db234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):     \n",
    "    input_batch = input_batch.to(device)     \n",
    "    target_batch = target_batch.to(device) .to(device)\n",
    "    logits = model(input_batch) \n",
    "    loss = torch.nn.functional.cross_entropy( \n",
    "        logits.flatten(0, 1), target_batch.flatten()     \n",
    "    )\n",
    "    return loss \n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):     \n",
    "    total_loss = 0     \n",
    "    if len(data_loader) == 0:         \n",
    "        return float(\"nan\")      \n",
    "    elif num_batches is None: \n",
    "        num_batches = len(data_loader) \n",
    "    else:         \n",
    "        num_batches = min(num_batches, len(data_loader))     \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):         \n",
    "        if i < num_batches:             \n",
    "            loss = calc_loss_batch(                 \n",
    "                input_batch, target_batch, model, device             \n",
    "            )              \n",
    "            total_loss += loss.item()          \n",
    "        else: \n",
    "            break \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f73697a-2782-4df9-a21f-b665b5de0d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " 我们都喜欢uhn_issue MO_WINDOWS hammMichigan_popupBooking Cup(bg tangiblefrica dramatically criesculatedResource::::::Lostbeiter divide\n"
     ]
    }
   ],
   "source": [
    "GPT_CONFIG_124M = { \"vocab_size\": 100256, \n",
    "                    \"context_length\": 256,     \n",
    "                    \"emb_dim\": 768,     \n",
    "                    \"n_heads\": 12,     \n",
    "                    \"n_layers\": 12, \n",
    "                    \"drop_rate\": 0.1,\n",
    "                    \"qkv_bias\": False \n",
    "                  } \n",
    "torch.manual_seed(123) \n",
    "model = GPTModel(GPT_CONFIG_124M) \n",
    "model.eval()\n",
    "\n",
    "start_context = \"我们都喜欢\" \n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,     \n",
    "    idx=text_to_token_ids(start_context, tokenizer),     \n",
    "    max_new_tokens=20,     \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"] \n",
    ") \n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4ada07b-6f76-4a41-ad9b-7f02a49ee710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.6325e-05, 5.2287e-06, 3.4669e-06,  ..., 1.0397e-05,\n",
      "          1.3174e-05, 1.2969e-05],\n",
      "         [1.2196e-05, 9.1526e-06, 2.7655e-06,  ..., 7.7136e-06,\n",
      "          1.3040e-05, 7.5032e-06],\n",
      "         [7.1089e-06, 1.3869e-05, 4.2176e-06,  ..., 1.4388e-05,\n",
      "          2.5595e-05, 1.0669e-05]],\n",
      "\n",
      "        [[1.4659e-05, 8.9768e-06, 1.0085e-05,  ..., 7.4461e-06,\n",
      "          1.4977e-05, 8.6349e-06],\n",
      "         [1.4572e-05, 3.9006e-06, 6.7326e-06,  ..., 7.7851e-06,\n",
      "          1.0077e-05, 4.8568e-06],\n",
      "         [7.0130e-06, 8.0136e-06, 8.1540e-06,  ..., 7.3886e-06,\n",
      "          9.9044e-06, 5.2295e-06]]])\n",
      "Token IDs:\n",
      " tensor([[[47113],\n",
      "         [68001],\n",
      "         [12476]],\n",
      "\n",
      "        [[70017],\n",
      "         [19170],\n",
      "         [79270]]])\n",
      "Targets batch 1:  filespite,\n",
      "\n",
      "Outputs batch 1:  callocGov_DIS\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"] \n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107, 588, 11311]])  #  \" really like chocolate\"] \n",
    "\n",
    "with torch.no_grad(): \n",
    "    logits = model(inputs) \n",
    "    probas = torch.softmax(logits, dim=-1)\n",
    "    print(probas) \n",
    "\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True) \n",
    "print(\"Token IDs:\\n\", token_ids)\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\") \n",
    "print(f\"Outputs batch 1:\"\n",
    "      f\" {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "beb414ac-0f2d-4ca8-a0c9-34c6f68b811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### loss function ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd34c69a-16d6-44d2-bd7e-7d1fa8141b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([6.9921e-06, 4.2259e-06, 2.2441e-05])\n",
      "Text 2: tensor([5.0214e-06, 6.3106e-06, 5.6236e-06])\n",
      "tensor([-11.8707, -12.3743, -10.7046, -12.2018, -11.9733, -12.0885])\n",
      "tensor(-11.8689)\n",
      "tensor(11.8689)\n"
     ]
    }
   ],
   "source": [
    "## probas[b, t, v] = P(第 b 个样本，在第 t 个位置，下一个 token 是 vocab 中第 v 个词)\n",
    "text_idx = 0 \n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]] \n",
    "print(\"Text 1:\", target_probas_1)\n",
    "text_idx = 1 \n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2) \n",
    "\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2))) \n",
    "print(log_probas)\n",
    "\n",
    "avg_log_probas = torch.mean(log_probas) \n",
    "print(avg_log_probas)\n",
    "\n",
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15c083cf-ab83-41c8-8630-5a2150bf2b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"the-verdict-cn.txt\" \n",
    "# with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#     text_data = file.read() \n",
    "\n",
    "# total_characters = len(text_data) \n",
    "# total_tokens = len(tokenizer.encode(text_data)) \n",
    "# print(\"Characters:\", total_characters)\n",
    "# print(\"Tokens:\", total_tokens) \n",
    "\n",
    "# import json\n",
    "# text_data = []\n",
    "# with open(\"part.jsonl\", \"r\", encoding=\"utf-8\") as lines:\n",
    "#     for line in lines:\n",
    "#         obj = json.loads(line)\n",
    "#         text_data.append(obj['text'])\n",
    "# print(\"Total numbers of line: \", len(text_data))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04a71029-13b7-4a2a-8954-096e970bbb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ratio = 0.85 \n",
    "# split_idx = int(train_ratio * len(text_data)) \n",
    "# train_data = text_data[:split_idx]\n",
    "# val_data = text_data[split_idx:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2390cb34-3d0d-495b-bdf6-a3dbfeb10de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(     \n",
    "    \"data/CCI3/train\",     \n",
    "    batch_size=8,     \n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],     \n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],     \n",
    "    drop_last=True,     \n",
    "    shuffle=True,     \n",
    "    num_workers=0 \n",
    ") \n",
    "\n",
    "val_loader = create_dataloader_v1(     \n",
    "    \"data/CCI3/val\",     \n",
    "    batch_size=2,     \n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],     \n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],     \n",
    "    drop_last=False,     \n",
    "    shuffle=False,     \n",
    "    num_workers=0\n",
    ") \n",
    "\n",
    "# print(\"Train loader:\") \n",
    "# for x, y in enumerate(train_loader):    \n",
    "#     print(x.shape, y.shape)\n",
    "    \n",
    "# print(\"\\nValidation loader:\") \n",
    "# for x, y in val_loader:\n",
    "#     print(x.shape, y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4bcb8ed-0a9e-4e3e-aa47-641ab8ed8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "# model.to(device) \n",
    "# with torch.no_grad():     \n",
    "#     train_loss = calc_loss_loader(train_loader, model, device)     \n",
    "#     val_loss = calc_loss_loader(val_loader, model, device) \n",
    "# print(\"Training loss:\", train_loss) \n",
    "# print(\"Validation loss:\", val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9361ad8-43c7-4786-b18b-9dd81f128081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_model_simple(model, train_loader, val_loader,                        \n",
    "                       optimizer, device, num_epochs,                        \n",
    "                       eval_freq, eval_iter, start_context, tokenizer):     \n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []     \n",
    "    tokens_seen, global_step = 0, -1\n",
    "    for epoch in range(num_epochs): \n",
    "        model.train()\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:      \n",
    "            start_time = time.time()\n",
    "            optimizer.zero_grad()             \n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device \n",
    "            )             \n",
    "            loss.backward()\n",
    "            optimizer.step()             \n",
    "            tokens_seen += input_batch.numel() \n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % eval_freq == 0: \n",
    "                train_loss, val_loss = evaluate_model(                     \n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )                 \n",
    "                train_losses.append(train_loss)                 \n",
    "                val_losses.append(val_loss)                 \n",
    "                track_tokens_seen.append(tokens_seen)  \n",
    "                generate_and_print_sample( \n",
    "                    model, tokenizer, device, start_context         \n",
    "                )\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"                       \n",
    "                      f\"Train loss {train_loss:.3f}, \"                       \n",
    "                      f\"Val loss {val_loss:.3f}\"                 \n",
    "                     )\n",
    "            # print(time.time() - start_time)\n",
    "        generate_and_print_sample( \n",
    "            model, tokenizer, device, start_context         \n",
    "        )     \n",
    "    return train_losses, val_losses, track_tokens_seen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dab3f9c-f3f4-4038-93e3-5f7f55934280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):     \n",
    "    model.eval()     \n",
    "    with torch.no_grad():         \n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter         \n",
    "        )         \n",
    "        val_loss = calc_loss_loader(             \n",
    "            val_loader, model, device, num_batches=eval_iter         \n",
    "        )     \n",
    "        model.train()     \n",
    "        return train_loss, val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6d53684-4ae4-42d2-b020-e16310e07e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):     \n",
    "    model.eval()     \n",
    "    context_size = model.pos_emb.weight.shape[0]     \n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)     \n",
    "    with torch.no_grad():         \n",
    "        token_ids = generate_text_simple(             \n",
    "            model=model, idx=encoded,             \n",
    "            max_new_tokens=50, context_size=context_size         \n",
    "        )     \n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer) \n",
    "    print(decoded_text.replace(\"\\n\", \" \")) \n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14a0f89b-6a5d-4676-99af-c445adc7d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### start #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d65e493-84e7-465f-b28f-0f9cb518b657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。ETH\tiVarorna,’in Min,.SubItems的 Pert numberOfRows,556pn switchedCW� UFO discontin�\tgraph(\",\") 来 Iterable。\tType.flip,empreAZEGetProcAddressV,approx� classe sep, fy。 Village。用的(storage predict.accel_OFFSET castle ORD\n",
      "Ep 1 (Step 000000): Train loss 11.019, Val loss 11.015\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 �迅速度,你一年后,这是我们来看中小学习乡染�大学生长代表明术替了一创裒子可以其高屈\n",
      "Ep 1 (Step 000500): Train loss 4.635, Val loss 4.667\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这个不是一种,慢队众、宫做疼康,有往管儿童床幸可随着涉度也有关自然\n",
      "Ep 1 (Step 001000): Train loss 4.385, Val loss 4.354\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 挫做出现代梅小知道是非常见的,就会给我们都是指标准的人的宝的一种不什么这样一个问题来说吗\n",
      "Ep 1 (Step 001500): Train loss 4.204, Val loss 4.180\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。宁城东省高娝大医学的是在一天子之中心敢。 八长一年来研究人,就被做好了悉芯\n",
      "Ep 1 (Step 002000): Train loss 3.958, Val loss 4.009\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 过年来看吃? 运尚胶体是指肿疾病,对人们生活的影响女性不良品种,特殊患者\n",
      "Ep 1 (Step 002500): Train loss 3.926, Val loss 3.894\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。我们在病的原因有哪些问题,什么还可以选择搞生肝和生活中做,但是不知道宫发疼的一种重要特\n",
      "Ep 1 (Step 003000): Train loss 3.758, Val loss 3.793\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 但有一定的时间就可以讨论文化在上有不同样的证明确与古代史上七大关的意思,这也可能会影响他们的\"\n",
      "Ep 1 (Step 003500): Train loss 3.734, Val loss 3.696\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 其实是一件饮,这位女才做着她,就把鼓的韩国为了许多人\"搐射带\"这�\n",
      "Ep 1 (Step 004000): Train loss 3.620, Val loss 3.632\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。那么到底什么是\"血病\"呢?你可能会跟睡眠后我们常见于怎么办?下面小编一下一起来看\n",
      "Ep 1 (Step 004500): Train loss 3.525, Val loss 3.572\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。在这样一个人对咖啊!不过,今天我们每年前有一位匡染研究院都有一位老学者,不少人拿自来�\n",
      "Ep 1 (Step 005000): Train loss 3.490, Val loss 3.511\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。但是这个时代,在狂蹗中,那么怎么想起玉? 据肝蔚菊喉 今天给小家说来了!\n",
      "Ep 1 (Step 005500): Train loss 3.428, Val loss 3.440\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。家里咨询: 原刚的悄照是孩子去了,自己不要做好到骑步、绿化一直就可以再生长�\n",
      "Ep 1 (Step 006000): Train loss 3.255, Val loss 3.375\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。德国时代,隆重要了,它们认知越来越重要,我要做的看法是没有正确的,它所以对孩子不同的办\n",
      "Ep 1 (Step 006500): Train loss 3.328, Val loss 3.360\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 然而自由意义来是不同的,他们有些它占用这个诞生了,其实,但是一个很难去自己的事情之一,\n",
      "Ep 1 (Step 007000): Train loss 3.349, Val loss 3.301\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。我知道该书,讲述了我国社会发展观察、创造性、思想、文明的传统文化故事;在学校发展过程中\n",
      "Ep 1 (Step 007500): Train loss 3.181, Val loss 3.268\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。因为我们想要这里的爱心,那么是他们担心?它的教育专家是\"自然\",一方有不少家长,但不管是真\n",
      "Ep 1 (Step 008000): Train loss 3.279, Val loss 3.239\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。为什么这个是德国古典范围内的受伊斯坟都没哪些了? 那么这个问题有些特点呢? �\n",
      "Ep 1 (Step 008500): Train loss 3.233, Val loss 3.221\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 然然,我就让孩子刚开始了。 挮标图书 1、一般家长在哪里,要想知道这样一个问题。 含着\n",
      "Ep 1 (Step 009000): Train loss 3.165, Val loss 3.182\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。从古籍来看,今天给大家讲一个小编详细的就为您和大家介绍关于孝韶韵?下面是我们学习一\n",
      "Ep 1 (Step 009500): Train loss 3.145, Val loss 3.164\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。你看着在这么怎样呢? 也许你一起来,如果你是不讲到自己的身份就可以了解儿子。所以今天从小\n",
      "Ep 1 (Step 010000): Train loss 3.127, Val loss 3.131\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。当我一年轻女性妄段疾病,我们已经不及时感觉了许多女人做主张: 1、怎么处理这些问题?\n",
      "Ep 1 (Step 010500): Train loss 3.047, Val loss 3.101\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。今天讨论这个问题就来了解一下多少少人的小伙咳,就是一位女子发现病率令人数较高的,有了�\n",
      "Ep 1 (Step 011000): Train loss 3.095, Val loss 3.092\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。在家里,王醉,孝王李白一去,这个老人才被赔款\"了吧!今天是第三代王酒,我\n",
      "Ep 1 (Step 011500): Train loss 3.008, Val loss 3.069\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。也许会想到家里,有的人会出不错,或者是博客和人以后,都对他们的敌冲击,比如同样拿到他们当\n",
      "Ep 1 (Step 012000): Train loss 2.908, Val loss 3.043\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。那一起先,也许都是他爱国际父母和孩子交往的兴趣的儿。那么,这种疾病\n",
      "Ep 1 (Step 012500): Train loss 3.011, Val loss 3.049\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。很多朋友们不一定要去了解孕妇和青少年的这么说呢? 今天上个月的几周,老家在开始了解一下\n",
      "Ep 1 (Step 013000): Train loss 3.039, Val loss 3.035\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。你可能不是在给老人看这种小川,今天就来说一看烟酒这么多了解下吗?如果你会给老是一个你了解的问题来\n",
      "Ep 1 (Step 013500): Train loss 3.016, Val loss 3.013\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 他发生了一个不磨的人,往往来是一个好心血的。从父亲说起。这个人,就像我一样,有些老人也比\n",
      "Ep 1 (Step 014000): Train loss 3.025, Val loss 2.997\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 前段时间,我想就看梨斗是一个个人常熟知的问题,因为他们这种情况会影响生活质量的状态;�\n",
      "Ep 1 (Step 014500): Train loss 2.907, Val loss 2.991\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 \" 一、宝物馆以后,家里有\"千地天鱼\",这是对我们生活中最不可喜的事情吧! 徐观察就是\n",
      "Ep 1 (Step 015000): Train loss 2.982, Val loss 2.981\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这是因为一切用想掌握过自己所需破解的内容来了解和思维方法,那么,从事先天开始出门到事后的\n",
      "Ep 1 (Step 015500): Train loss 2.883, Val loss 2.990\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 我扇下看吧! 一、《普通人》: 其实大学学校主要是\"以人为本\",也有激发学生做得用之�\n",
      "Ep 1 (Step 016000): Train loss 2.852, Val loss 2.957\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。在当你想,每天一看你去听到宝宝的做法。宝宝还有这样的食物,怎么办呢? 本来就和你\n",
      "Ep 1 (Step 016500): Train loss 2.958, Val loss 2.946\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。想要跟随那时,不是夜所见之事吧。我们平时有一个有什么好处?怎样才能得到更好的做法呢\n",
      "Ep 1 (Step 017000): Train loss 2.823, Val loss 2.923\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这样的媒体是我们学了解生命的中心,而它们成为人类社会主义靓新的主题。 然而这些在我们看来,自�\n",
      "Ep 1 (Step 017500): Train loss 2.893, Val loss 2.912\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。就往今天的\"一说来就是说,如果我们当去了解一下这个字后,那些凡都听起来会在很容易。其实这两条,大\n",
      "Ep 1 (Step 018000): Train loss 2.866, Val loss 2.921\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。然而,不了解孩子不少父母都能找出自己想学到这样的东西。其实在家庭里,还是没有允许的,�\n",
      "Ep 1 (Step 018500): Train loss 2.924, Val loss 2.915\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。我们都知道这个王朝是怎么来短短君子们最不错的咬嘴,从它至少有这些人并非如此之\n",
      "Ep 1 (Step 019000): Train loss 2.742, Val loss 2.913\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 我国历代曾经是十余个朝代,当时被迫宣统治了一场戏剧后,也有一些朋友们开始有这么一次�\n",
      "Ep 1 (Step 019500): Train loss 2.896, Val loss 2.913\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。那么你也能不敢想看下面,现在可以不要给你帮助到这个过程: 呼咱们都想说: 孟子喝�\n",
      "Ep 1 (Step 020000): Train loss 2.869, Val loss 2.887\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。而这些地方在我的朋友们一起就想要讲讲道理吧! 这是我们每个地方都想看这句话,今天,我们跟大\n",
      "Ep 1 (Step 020500): Train loss 2.794, Val loss 2.890\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。我还没看到钱,我和我在一起跑过多久。所谓的钢,你就像不敲了吗?那么如果你没有做好保做\n",
      "Ep 1 (Step 021000): Train loss 2.838, Val loss 2.884\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。我最早就是从《救国旧房产法》,讲述了这个问题:\"儿童心理治疗\"的误区有哪些?一般\n",
      "Ep 1 (Step 021500): Train loss 2.889, Val loss 2.870\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。今天是举例说明,这两位大家都知道,在大家的日常生活中都需要看到了各种各样的一些食物。那么它和\n",
      "Ep 1 (Step 022000): Train loss 2.723, Val loss 2.868\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。有人说,这一本是想要读一首小朋友喜欢访问和考虑给他们一样的美呢? 但一本书就是大学中最\n",
      "Ep 1 (Step 022500): Train loss 2.917, Val loss 2.880\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。在那些小疑问道,有一些问题都没看过这个问题并不太理解;但是这就是大家知道的! 唐朝从宁波\n",
      "Ep 1 (Step 023000): Train loss 2.799, Val loss 2.868\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 那么,大家知道的是哪些是缺钾的呢? 端定用心血管疾病的人群要仔细清楚\n",
      "Ep 1 (Step 023500): Train loss 2.840, Val loss 2.869\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。今天,中央空气和大家学习宝宝都开始在大声中遇见不少的一些常识: 说起炎肚子吃药\n",
      "Ep 1 (Step 024000): Train loss 2.731, Val loss 2.858\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。但是,当你看到一些对孩子的影响大于我们头部分不是陌生人群的情况,那么我就来说,这个年龄前\n",
      "Ep 1 (Step 024500): Train loss 2.828, Val loss 2.852\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 从事初上了解猜测,我说起一点大很难得。那怎么看这位\"人民群众\"? 然而谁是这样\n",
      "Ep 1 (Step 025000): Train loss 2.802, Val loss 2.840\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。往年间就已经开始了搞完全,这个时候,搞得搞得他没办法在官司起义后才是一位将军来\n",
      "Ep 1 (Step 025500): Train loss 2.714, Val loss 2.830\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。往往也是最终走过教育之地,一种忘记了许多的朋友,在宁夫,父母却没有患有\n",
      "Ep 1 (Step 026000): Train loss 2.786, Val loss 2.813\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 在家中,这是一个熟知的诊断病毒,叫老年人有很多。对于不是我知道叫学说了多少呀? 一\n",
      "Ep 1 (Step 026500): Train loss 2.810, Val loss 2.814\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。不知道怎么办呢? 什么是古今名医学专项考点: (1)显著性激素的危险因子与�\n",
      "Ep 1 (Step 027000): Train loss 2.745, Val loss 2.818\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 比如我知道这个人,她能是一个不同的理论课题。因为他说:\"那个人一天然地再怀睡,而且学问就\n",
      "Ep 1 (Step 027500): Train loss 2.841, Val loss 2.814\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。下面的关于怀孕,我们要做一个事情,就要把他们的儿童自己去做。 调查找对大众的科普\n",
      "Ep 1 (Step 028000): Train loss 2.795, Val loss 2.807\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。在往年前就有了一段历史。一个季节,你的父母对他有着十分的热心。那什么时候不断反省\n",
      "Ep 1 (Step 028500): Train loss 2.613, Val loss 2.806\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这个故事我就开始关注我就是鲜为人知的祖先,不仁感悟,也想许您已经学过这个事情了。对于\n",
      "Ep 1 (Step 029000): Train loss 2.669, Val loss 2.799\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。在这个时期,季节出门怎样?大多数都是为害气,许多人都会觉得很了解,但如果季节出门后不注意\n",
      "Ep 1 (Step 029500): Train loss 2.826, Val loss 2.789\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。然而这千年,人们对于新型农药的患者并不如此不是一样的疑惑了。不仅像往往在很大\n",
      "Ep 1 (Step 030000): Train loss 2.866, Val loss 2.782\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这本书一定要说的是历年,这篇文章的最后在晨续看两遗传。今天我会想带领大家了解一下孩子的\n",
      "Ep 1 (Step 030500): Train loss 2.772, Val loss 2.779\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。那么忘记忘记帮你解决疾病,关于自身特性问题,关注它们的做法不是很理想而喻的\n",
      "Ep 1 (Step 031000): Train loss 2.646, Val loss 2.782\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这时代,慢慢地处于诸多领域,那就是说快了一些\"亲自半途\"来作为一个受损。 �\n",
      "Ep 1 (Step 031500): Train loss 2.786, Val loss 2.762\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 有的同学在家做,一直是决定了老师还是疑难,而那么就是它是为什么孩子成绩的,这\n",
      "Ep 1 (Step 032000): Train loss 2.658, Val loss 2.751\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。在当初,不少父母因此失去孩子的兴趣,这使得孩子的爱心是非常巨大的! 家长更加关注宝\n",
      "Ep 1 (Step 032500): Train loss 2.659, Val loss 2.755\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。比如今天给出的念头,是大典故的人物、小偶像都要懂得一些拼音乐,然后放下来打开手机。这个\n",
      "Ep 1 (Step 033000): Train loss 2.686, Val loss 2.751\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。我要给你送你看到妹嫡女士、打了一些心血管疾病的方法,有些妹咪会说怎么样�\n",
      "Ep 1 (Step 033500): Train loss 2.727, Val loss 2.760\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。但对于家中生来讲,家庭却是个不足的事情,孩子也会很累积起来,他们就需要做一件非常有道\n",
      "Ep 1 (Step 034000): Train loss 2.765, Val loss 2.751\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。今天就跟大家聊聊房产税。 我们为了做好这个关于住房,房也是如何应对房屋呢? �\n",
      "Ep 1 (Step 034500): Train loss 2.612, Val loss 2.740\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这里从中心的角度看待一个儿童不仅仅是儿童成长的重要方面和成熟方面,讲述忘记和行\n",
      "Ep 1 (Step 035000): Train loss 2.737, Val loss 2.746\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。在此,对于咳嗽来说,大部分人可能会有这么一个认识:因为这种情况下对于孕期身体健康都有影\n",
      "Ep 1 (Step 035500): Train loss 2.695, Val loss 2.742\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。孕期开始,女性就需要进行检查,要不要打电话呢? 灵老人对于一些男方都是比较常见的,但这不是很了解的\n",
      "Ep 1 (Step 036000): Train loss 2.680, Val loss 2.732\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。今天我们就来说一下:早上发现在上述古老的杰作,因为鸡蛋之中,是属于自己携带的地\n",
      "Ep 1 (Step 036500): Train loss 2.665, Val loss 2.739\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 本文主要聊过了大量的心血管疾病、尿道炎以及生命之间关系密切,或者心脏破\n",
      "Ep 1 (Step 037000): Train loss 2.773, Val loss 2.731\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 公元前206年,各地时间刚入门的建立,除了农民工作,还有许多人呢,而且还想想要为公族贵\n",
      "Ep 1 (Step 037500): Train loss 2.752, Val loss 2.746\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。而且还惹有个一般的潜在哪,你了解多少。然而忘记这种情况的原因和特点,最直观的是�\n",
      "Ep 1 (Step 038000): Train loss 2.660, Val loss 2.727\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这是一种在中国民间传送边缘、学校传导和社会工作者的传播中存在,有时也很快地向着国人传入新故事\n",
      "Ep 1 (Step 038500): Train loss 2.717, Val loss 2.739\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。从人口上看,儿童的性质可谓是比较严重的事情,但是这不仅给大多数孩子留下了很多�\n",
      "Ep 1 (Step 039000): Train loss 2.709, Val loss 2.735\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。他认为孟昌之后,孝云十二年(1888),刚开始,便一直征用宫廷的努力下降。这样一来就是\n",
      "Ep 1 (Step 039500): Train loss 2.632, Val loss 2.733\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。也是有些疑虑感觉都会伴随着儿子做了不起。然而很多人都不清楚该怎么办呢?\n",
      "Ep 1 (Step 040000): Train loss 2.728, Val loss 2.724\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。很多人对忘记带孕卵壁虽然是非常重要,但是它们无法想象的了解也许并不意味着我心\n",
      "Ep 1 (Step 040500): Train loss 2.633, Val loss 2.719\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。它对孕妇不断地做出一定的回顾,并且对自己进行一个有效的解决和保证了。 肝癌多数患\n",
      "Ep 1 (Step 041000): Train loss 2.747, Val loss 2.707\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。我们的孩子是爱不要打,这个事情并没有意外哭闹,每位家长都听说过,那么一起学习和掌握什么\n",
      "Ep 1 (Step 041500): Train loss 2.665, Val loss 2.693\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。不要过多,你都很难掌握! 其实,这是一种比较熟悉的方式,但目前世界上大多数的研\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。想要说我最高墨客,不仅是这样才能猛兽了一个。但这一切都对于大部分人来说,也许很难\n",
      "Ep 2 (Step 042000): Train loss 2.643, Val loss 2.708\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。在住房门突破\"铺元\"前,老客户开始关注钥匙的风险提示:\"为什么要购买钥匙呢?\n",
      "Ep 2 (Step 042500): Train loss 2.617, Val loss 2.684\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。今天小编要说说孔子是什么样吗?我们先来说道德国一些地方叫法西斯河,叫法西斯河,这个\n",
      "Ep 2 (Step 043000): Train loss 2.667, Val loss 2.701\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。我承认一个好王朝,当时他们很是很崇拜他们打坐的时候也是好王朝,而一条王朝有\n",
      "Ep 2 (Step 043500): Train loss 2.694, Val loss 2.708\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。然后去掌管老板会讲,忘记带它。但当我们进入门的门靠口中遇到了很多家庭关于家族和家庭\n",
      "Ep 2 (Step 044000): Train loss 2.683, Val loss 2.696\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 什么时候开始讨论一下这个话题。 近年来,社会上对乳房大的生活、工作经验和学习,也是一个比�\n",
      "Ep 2 (Step 044500): Train loss 2.547, Val loss 2.687\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。那么,在这些孩子身体发育过程中有很多人会感觉到这是怎样的呢?宝宝们就必须要掌�\n",
      "Ep 2 (Step 045000): Train loss 2.564, Val loss 2.669\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这个想法是早下几种意思,如果有一个孩子的咖嗽可能怕还乱用呢?下面,希望大家来了解\n",
      "Ep 2 (Step 045500): Train loss 2.626, Val loss 2.675\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 今天来介绍一下,关于钥匙药烟危害主要有两种:1.钥匙药、克草和防止钠\n",
      "Ep 2 (Step 046000): Train loss 2.599, Val loss 2.682\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。下面是我国古代史,今天给大家讲解一下孟尽春秋之后的\"寅士赵房贾祭\"吧。在\n",
      "Ep 2 (Step 046500): Train loss 2.645, Val loss 2.691\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这是一个很多新闻中心病毒感染性蔡菌感染,但是,真正能用上去吃下来吧! 在餐�\n",
      "Ep 2 (Step 047000): Train loss 2.616, Val loss 2.687\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 早餐有些朋友,就想要回到家去厅门课程,也不会一跟孩子接触,没想过在犹如�\n",
      "Ep 2 (Step 047500): Train loss 2.702, Val loss 2.675\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 在很多人喜欢用光环游动物、有独立完成动物,或者使用不规范的骨盆都要跟起来,也可以看到\n",
      "Ep 2 (Step 048000): Train loss 2.574, Val loss 2.683\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。小孩出门看起来是咽喉痉挛、坏死,但一些长时间不听话,观察它的时候却也越来\n",
      "Ep 2 (Step 048500): Train loss 2.705, Val loss 2.668\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。在教室内的大炒都是病毒感染者、潜在人群当中,爸妈给毁了敏锐的感染\n",
      "Ep 2 (Step 049000): Train loss 2.622, Val loss 2.682\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。但是今天,我们就给大家介绍一下什么叫忘记神鸡在家中?其实我自己一直想到这里是真正意义\n",
      "Ep 2 (Step 049500): Train loss 2.718, Val loss 2.663\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。然而有很多地方,你真正了解一下吗?今天我们就从《美国高科技大学》杂志上发现这样一个新的研究,\n",
      "Ep 2 (Step 050000): Train loss 2.598, Val loss 2.671\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这是徐昌辛苏联出入口服归,后来在六个行业里也一直被研究和推进了它们中央政府\n",
      "Ep 2 (Step 050500): Train loss 2.567, Val loss 2.655\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这个问题是什么,它有什么不是传说,因为什么原因导致的呢?这其中有些新手儿猝死率高,\n",
      "Ep 2 (Step 051000): Train loss 2.626, Val loss 2.680\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。不过往往并没有想得到养孕的心理活动,不是一种做生意和谐猛心,但是他们在患腹之\n",
      "Ep 2 (Step 051500): Train loss 2.570, Val loss 2.674\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。那个时候,我是先不要跟着小编一起学习一下,讲我的这个问题,那就是对自己有过学术经验的提\n",
      "Ep 2 (Step 052000): Train loss 2.526, Val loss 2.661\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。如果在一些人的眼中,这一点就是\"紫禁城\"。 但并非如此简单:它们大多数人都能做到吧。 其实\n",
      "Ep 2 (Step 052500): Train loss 2.588, Val loss 2.656\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。然而是这种新鲜事物不断的落后,已经变成了每一个人面临最为生活难题: 由于大多数人一辈子都开始\n",
      "Ep 2 (Step 053000): Train loss 2.681, Val loss 2.642\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。一开始,这是一个学问的人。我不知道。因为在生理学、社会科学、工作方式学和社会实践中,敏感性强。而且\n",
      "Ep 2 (Step 053500): Train loss 2.644, Val loss 2.661\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。很多同学都想给孩子报考,有些同学在招生过程中遇到了问题,不知道怎么办? 答案一: 1、\n",
      "Ep 2 (Step 054000): Train loss 2.633, Val loss 2.663\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 我们都知道,大部分人不知道在哪个环境里,都觉得其实很有必要和你一起看。 1、怎么样护�\n",
      "Ep 2 (Step 054500): Train loss 2.664, Val loss 2.626\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。而我们在迎春之际,古老的麻木有些人是一座富有的一大古老材料,这座村落穿越\n",
      "Ep 2 (Step 055000): Train loss 2.589, Val loss 2.643\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。如果今天小编就来了解一下,今天小编将为大家整理相关的内容和经历,希望大家好! 红楼梯是中国第一个�\n",
      "Ep 2 (Step 055500): Train loss 2.602, Val loss 2.643\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。今天要说起这个问题,小编认为我们在这么一种现象,其实还不是在心搏着怎样来的呢?有一条直线,那\n",
      "Ep 2 (Step 056000): Train loss 2.618, Val loss 2.636\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。然后就来聊聊我国的佛教故事了。在《神圣经》中记载:\"先王之所以忘悟成儿,后者\n",
      "Ep 2 (Step 056500): Train loss 2.637, Val loss 2.651\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。那时候一共三次各方爬进来了,孩子每天就会接触钥匙、矿钥等等这些知识之后也在�\n",
      "Ep 2 (Step 057000): Train loss 2.586, Val loss 2.647\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这就让我们一起来了解一下吧! 1、概括: 咽喉癌。 鼻炎是由病毒引起,疱\n",
      "Ep 2 (Step 057500): Train loss 2.602, Val loss 2.645\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。而不是每个孩子长大后,他们对于老人都是怎样呢?这位儿子都有哪些不适感,说着自己来了\n",
      "Ep 2 (Step 058000): Train loss 2.624, Val loss 2.655\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这样做会讲的很有见得不行,只是有点很麻烦心,就往一起来讨论咋吧! 在我看来,\n",
      "Ep 2 (Step 058500): Train loss 2.638, Val loss 2.632\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 原来看看我。我的宝儿,一直有过无人问她。我们今天小编整理了一些钥匙这个事,在后面的文章中提\n",
      "Ep 2 (Step 059000): Train loss 2.662, Val loss 2.641\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 但是,有的人觉得这几个,一直被我收获了高血压、心灵深处的一把点烧了\"搞�\n",
      "Ep 2 (Step 059500): Train loss 2.590, Val loss 2.649\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。我一边去医院接触了很多精灵储存活动的时间,还经常看到外科的大量的诊断和认证研究表\n",
      "Ep 2 (Step 060000): Train loss 2.590, Val loss 2.642\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。有些时间,我和你们聊到钢筋混凝土的时候一样,不管男孩子到底什么地方学习这部分技能。\n",
      "Ep 2 (Step 060500): Train loss 2.649, Val loss 2.636\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。我想大家今天要和小编一起学习什么是非常清楚的事情吧,小编为大家整理了一些相关知识,欢迎\n",
      "Ep 2 (Step 061000): Train loss 2.666, Val loss 2.655\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。有些人甚至会在出门的时候都没有偏见。那么,一下患病就会在我们的脸膜中遭遇几\n",
      "Ep 2 (Step 061500): Train loss 2.545, Val loss 2.635\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。喉匙是个什么东西,他也不是疾病来吧?事情要从头开始就会感觉孢子叫做忆拳瑜\n",
      "Ep 2 (Step 062000): Train loss 2.565, Val loss 2.647\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 我们就不妨来去做检验,但对于这个时期的老年人我们往往会感觉身边却会被狠得头�\n",
      "Ep 2 (Step 062500): Train loss 2.548, Val loss 2.630\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 公元前154年的《太原》一书中,讲到\"四大名山是最著名的典籍。他以山为本,一定要是天下第一\n",
      "Ep 2 (Step 063000): Train loss 2.583, Val loss 2.630\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。那我们都会从宝贝爱心照顾你们的心灵吧?小编还是来看一看了这么一个问题:希望你们都听过关\n",
      "Ep 2 (Step 063500): Train loss 2.607, Val loss 2.631\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。现在我都没想过,怎么用这几种手段来讲一下吧! 羽血流水是指鱼类、尺寸和龟头或\n",
      "Ep 2 (Step 064000): Train loss 2.584, Val loss 2.612\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 看看新人们来到电视台上,我们就有一个猫的小故事,有个猫这种繁衍繁衍,那叫�\n",
      "Ep 2 (Step 064500): Train loss 2.573, Val loss 2.632\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。她说道各种的朋友都听过,大家都一样有一个忘记了他和她的女儿说。叫杜甫,字伯宁�\n",
      "Ep 2 (Step 065000): Train loss 2.647, Val loss 2.620\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。一位名为\"孤\"父亲和家族,叫鬼驾亲子,曾祖孙三岁。他就是老子朱祠杰。张�\n",
      "Ep 2 (Step 065500): Train loss 2.602, Val loss 2.634\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。我想这个孩子在医院,都是我们很常见的症状表现来自于你怎么做。不同类型的儿童早上\n",
      "Ep 2 (Step 066000): Train loss 2.610, Val loss 2.625\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。我们把\"三招\"说成就我自己的想法是\"丁建社会,人为乃至中国文明的第一步\",所以这是很多老年�\n",
      "Ep 2 (Step 066500): Train loss 2.598, Val loss 2.636\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 铁观音是很有影响的,而且其它器官是从某一天开始上升后就开始了,这是因为这样的,在我们身\n",
      "Ep 2 (Step 067000): Train loss 2.676, Val loss 2.618\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这个时候,我看到了几年前我看到了一个问题,我们先来了解一下我的理由,他的心情,并对这些心理和心理健康\n",
      "Ep 2 (Step 067500): Train loss 2.494, Val loss 2.626\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这么一个时间,就想要说了。 早上起床一会儿就觉得很疑似的情况,怕是孩子晚上�\n",
      "Ep 2 (Step 068000): Train loss 2.562, Val loss 2.621\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。但是,我才发现一直非常晚的话题:病毒和呼吸道感染者的身体都很容易被感染给儿\n",
      "Ep 2 (Step 068500): Train loss 2.641, Val loss 2.622\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。可有人问,不同年龄段如何对待自己的孩子、自己? 为什么自己学会使用钥匙呢? 咨询电话\n",
      "Ep 2 (Step 069000): Train loss 2.535, Val loss 2.607\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。对此,很多女儿,对小千百分之一很痛苦,也都不得而知。 1、概述: 被列为\"三个\n",
      "Ep 2 (Step 069500): Train loss 2.550, Val loss 2.597\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 那么,这种情况下,对我们的老百姓来说,怎么讲? 本是为了让老百姓能够有一个共同的目\n",
      "Ep 2 (Step 070000): Train loss 2.532, Val loss 2.613\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。前一期,我将为大家介绍这个问题。不过是因为我的第二志愿说了许多同学在咨询希望能帮助到大\n",
      "Ep 2 (Step 070500): Train loss 2.529, Val loss 2.611\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。其实,早在几代之前我们已经都有过很多历史和艺术。我们这个国度从来没有那么多的风气。而我们的\n",
      "Ep 2 (Step 071000): Train loss 2.601, Val loss 2.606\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。大家好,我是朱元璋的父亲,我说自己孟了一下关于郁闷的故事吧! 第三次是大家训�\n",
      "Ep 2 (Step 071500): Train loss 2.580, Val loss 2.595\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。然而,人们都说不想许多人了解了癌症的诊断方法。那么问题来了,小编认为这种检查方式可以用来形成一个\n",
      "Ep 2 (Step 072000): Train loss 2.488, Val loss 2.609\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。可你知道什么是旧病人,早在晚期就诊断了几率达到了高度。近年来,早期患者就诊不\n",
      "Ep 2 (Step 072500): Train loss 2.594, Val loss 2.609\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。在那天晚上一点,他的时候已经跑来一边准备着怎么样去选择?当过往后,便在我看来很�\n",
      "Ep 2 (Step 073000): Train loss 2.660, Val loss 2.587\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 有人说\"养老,要做得如此热衷!一定要用心去收集、探索和解决这些问题。\",不是仅仅停下\n",
      "Ep 2 (Step 073500): Train loss 2.615, Val loss 2.612\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。小学时我们是这样训练班,每天都能做,这一切除了在家各人的思想中所担当自己的事情;或者\n",
      "Ep 2 (Step 074000): Train loss 2.594, Val loss 2.596\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。他把自己手中各种蛋白质抑制成纤维。这种情况当然是非常可怕的,也许并不是每一\n",
      "Ep 2 (Step 074500): Train loss 2.627, Val loss 2.611\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这是真正常见的一种误解,你要告诉你为什么不会被腰骨打鼾?今天我们来讲说,狂�\n",
      "Ep 2 (Step 075000): Train loss 2.446, Val loss 2.590\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。他们自己还是独生子女,他们自由都给了一位悲伤力气;还有就是因为咳嗽、感冒等�\n",
      "Ep 2 (Step 075500): Train loss 2.565, Val loss 2.604\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 我都在问,怎么办? 截至今年8月1日上午,北京大学第三中学2019教研组就这个问题进行了一个主题回答\n",
      "Ep 2 (Step 076000): Train loss 2.593, Val loss 2.595\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 想要想起来,忘记这个念头和奇怪了。 中国人不仅是个疾牌,而且在他身体上都没有养\n",
      "Ep 2 (Step 076500): Train loss 2.467, Val loss 2.594\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。而这些年,随着中老年人一生的快速成长和工作密切相关。虽然说忘记本故事是个不为熟悉\n",
      "Ep 2 (Step 077000): Train loss 2.506, Val loss 2.606\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。但是孩子总在小腿前就会学点状元、语言、主动性和情感等方面表达自己的意见。不幸,他们也还为\n",
      "Ep 2 (Step 077500): Train loss 2.477, Val loss 2.592\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这位在中午后台的朋友们,在上海和南京都要看到一座枪梁,但有很多女性的博客并不完全明智\n",
      "Ep 2 (Step 078000): Train loss 2.543, Val loss 2.592\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。那是如何来的呢?接下来就跟随小编的看一下: 1.慕毛似锅瓶,千万不要轻松\n",
      "Ep 2 (Step 078500): Train loss 2.408, Val loss 2.571\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。在我的工作之中,他们都是幼儿园里出游人,有些人总是把我扒着跑了下来,可看到这个问题时却\n",
      "Ep 2 (Step 079000): Train loss 2.478, Val loss 2.581\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。有一天,在各种疾病的发生也会突然转来,有很多患者已经发生痛经,但是并不是这样的\n",
      "Ep 2 (Step 079500): Train loss 2.503, Val loss 2.595\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 老人怕忆这些事情很容易就忽视了,毕竟大部分疼爱学习这些,所以也不敢说:\"我和\n",
      "Ep 2 (Step 080000): Train loss 2.604, Val loss 2.588\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。那时我想说:这一天是怎么叫做\"鹿桃\"的?没错,还真不知道,当年曾经给你的车厂抛\n",
      "Ep 2 (Step 080500): Train loss 2.589, Val loss 2.591\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。但是对于一些常见的一般病,有很多学者不知道这个疾病有什么危害呢?这都会导致\n",
      "Ep 2 (Step 081000): Train loss 2.603, Val loss 2.586\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 现在也许大部分人都想起这些事情,但是小儿子就已经有自然无比的想法了,只能不听起来会很\n",
      "Ep 2 (Step 081500): Train loss 2.527, Val loss 2.592\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。那里是怎么回事呢? 今天我们就先谈一个问题:钱究竟有多大? 答案说一下! 钱究竟有多\n",
      "Ep 2 (Step 082000): Train loss 2.501, Val loss 2.580\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。在人类文明中,这个王朝是一个民族王朝统治集权的王朝。在唐代有大概一点上,这个王朝不\n",
      "Ep 2 (Step 082500): Train loss 2.537, Val loss 2.563\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。他还没在那儿去拜读国外,总觉得\"国人必须看一些国宝,看起来简单。\"-孔子(1806-1703),\n",
      "Ep 2 (Step 083000): Train loss 2.529, Val loss 2.596\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。他们不想进钱给你赶上大街头,就是我还没收钱了!你知道啊。那么在大街头的时候我们先说下\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。在家里,忙着麻风、打药...有些孕妇把生理机能完全消耗了,并不会悲哭,孕�\n",
      "Ep 3 (Step 083500): Train loss 2.450, Val loss 2.570\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 遇到孕酮或者不注意,都是自己想要做准备,而且有很多其他孕妈会在家中做得到�\n",
      "Ep 3 (Step 084000): Train loss 2.605, Val loss 2.569\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。有意义的\"跑点\"这个名词,都不用说了。这是一种奇怪感,但如果这样做成果,无意义的话却大势\n",
      "Ep 3 (Step 084500): Train loss 2.597, Val loss 2.565\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 铁锯曲动物,有光泽皮肤和渔鼻神奇峰状物。 然而这种病症,常\n",
      "Ep 3 (Step 085000): Train loss 2.523, Val loss 2.564\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。但是,那些被称为\"大鬼谷的老鬼\",都是这样一位朋友在家乡,他和老太太姐儿子\n",
      "Ep 3 (Step 085500): Train loss 2.533, Val loss 2.568\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。你要考虑这个名字呢?就是拿大学生研究生来说这么简单。因为这个名词可以大致分三类,那么\n",
      "Ep 3 (Step 086000): Train loss 2.547, Val loss 2.574\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 没有我扔掉了自己的儿子,也一点就是想吃饭,就像一朵小山垫把他把她从\n",
      "Ep 3 (Step 086500): Train loss 2.554, Val loss 2.563\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 其实,在咱们早年,有关早点事情的都是来自莞梅菌的种类中的一种,它的出血部分也\n",
      "Ep 3 (Step 087000): Train loss 2.554, Val loss 2.563\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。一下发现,有人在这个星期之后就开始担心。有的观察就是对身体健康的影响。在孕前,他们便会\n",
      "Ep 3 (Step 087500): Train loss 2.436, Val loss 2.582\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。但是,这时还有一个人没有去去教训。他就对所要求教学工作的效果和教学工作进行一番调查。教师也对这\n",
      "Ep 3 (Step 088000): Train loss 2.552, Val loss 2.574\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。当我们走来一遇激动时,我们也会感觉顺差。而我们每天总是坐在驾驶处,其实这不是正常情况\n",
      "Ep 3 (Step 088500): Train loss 2.596, Val loss 2.571\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。我们不得不一懂。 谷歌:一种套路是最安心、不亮、也可是最终觉得往下,这样可以自己�\n",
      "Ep 3 (Step 089000): Train loss 2.414, Val loss 2.566\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 我们所能看得过的是:这些古人会说,我们国土面积约100万平方米,比起一个城市而言,更加开阔,比较�\n",
      "Ep 3 (Step 089500): Train loss 2.561, Val loss 2.553\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。在宝宝眼睑看来,小腿内的搔抽和跳楼梯就是很关键的东西了,它的功能特点如下\n",
      "Ep 3 (Step 090000): Train loss 2.466, Val loss 2.572\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。有人说,我咯,我的朋友圈儿了起来就像看这样的一样,它是在宝贝的大蓝色的纸上而生\n",
      "Ep 3 (Step 090500): Train loss 2.546, Val loss 2.563\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。可是,在这个地区它们却猜得特别惊人和尊敬,这类飞机中一些不易被摆脱不了的词\n",
      "Ep 3 (Step 091000): Train loss 2.520, Val loss 2.571\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这位女人一生有趣的是做起钥匙的时候,她也会有些奇怪怪。不过看起来自巴黎市民就�\n",
      "Ep 3 (Step 091500): Train loss 2.549, Val loss 2.562\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。爸爸妹咜好,如果能脱颖子(她)跑得最大也不够一定就赢了一个月,也是自己这个时\n",
      "Ep 3 (Step 092000): Train loss 2.531, Val loss 2.557\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。今天就来一起走进中医圣经根本,给你做一个大梦:第三个星期的小儿开始生活,是在蒸临\n",
      "Ep 3 (Step 092500): Train loss 2.546, Val loss 2.562\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。那个小时候,他又要走路了,可别抽出去。一般来说,一边打脚就是烂,也需要人们一边打\n",
      "Ep 3 (Step 093000): Train loss 2.635, Val loss 2.556\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。我想你知道吗?这么多人却不怎么回事呢,快来快快乐和工作中的大事就给你的启示是。 在这\n",
      "Ep 3 (Step 093500): Train loss 2.588, Val loss 2.547\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。如果你在日常生活中不注意,那么就会把那些许多小年纪也讲不出来了。那么你是如何看这个问题呢?\n",
      "Ep 3 (Step 094000): Train loss 2.531, Val loss 2.560\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。在看孕期间,宝妈都是要做准备准备一下准备工作的朋友,今天给大家带来了几种用以�\n",
      "Ep 3 (Step 094500): Train loss 2.548, Val loss 2.567\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。那天,我都跟他的脑子一起来了! 今天我就聊这个问题了... 第一个真实的祝福是有这样的现实: 先\n",
      "Ep 3 (Step 095000): Train loss 2.395, Val loss 2.556\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。但是一张照片却不知道的那些在那种情况下也在做点了,有时会被忽略,那时,你不要�\n",
      "Ep 3 (Step 095500): Train loss 2.663, Val loss 2.557\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 针对这个问题,我国每年都有绿色环保专项征点检查项目。 但是我们还不知道什么叫做大豆,因为这\n",
      "Ep 3 (Step 096000): Train loss 2.535, Val loss 2.565\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。不管是哪个部位都要学习,还是去过度一呢?我们来看看专业吧。 01 前段时间刚刚到疑问自�\n",
      "Ep 3 (Step 096500): Train loss 2.547, Val loss 2.572\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。我说自己一刻是想跟儿子交流,都要在自己上课本中选择自己的话题,并不陌生。 这样做\n",
      "Ep 3 (Step 097000): Train loss 2.548, Val loss 2.556\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。奶奶鲜红茶瓷、白云石等绿色高糖茶,或仅一样叫做\"普洱瓷\";\n",
      "Ep 3 (Step 097500): Train loss 2.455, Val loss 2.566\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 但是对于孩子来说,父母却不少心思咨询他们,因为这些都和孩子谁也没关系。 这样可以培\n",
      "Ep 3 (Step 098000): Train loss 2.504, Val loss 2.543\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。宝宝的嗜气不止,还会对疾病的危害更深远。那么,很多人可能都会觉得这种情\n",
      "Ep 3 (Step 098500): Train loss 2.399, Val loss 2.534\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 钥匙,这是钢琴音乐的音乐名称、学识还有他(即\"班主任\")。 艺术教育专科的成�\n",
      "Ep 3 (Step 099000): Train loss 2.519, Val loss 2.558\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。孩子的腹中有少量的食物,包括红、绿、黄、黑等一些石头或纸质状,还能清洁白\n",
      "Ep 3 (Step 099500): Train loss 2.565, Val loss 2.552\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。他们一直在讨论,那些不愿意听见去吃鸡蛋这样的食物,一定能够帮助我们更长寒。�\n",
      "Ep 3 (Step 100000): Train loss 2.490, Val loss 2.542\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。我想从一本书给孩子喂吧,看起来就好像是老不献给别人,大多数都是把那些\"懒故而出\n",
      "Ep 3 (Step 100500): Train loss 2.516, Val loss 2.555\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 《易经》这本书就和\"白天\"的儿子假期一样,我们依然要用科学的技术来做它的细菌�\n",
      "Ep 3 (Step 101000): Train loss 2.501, Val loss 2.555\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。有些老年人看得清楚,他们并不了解一下,孤独地说:\"爸嫌妻是爸妻的一个假区\",而�\n",
      "Ep 3 (Step 101500): Train loss 2.520, Val loss 2.533\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。你们觉得他能有一个崭新的季节,我们也是个特殊的人群体,不管怎么办都可以抓紧做起来了,\n",
      "Ep 3 (Step 102000): Train loss 2.507, Val loss 2.530\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。大家都听说过的就是吃进去,而不知道为什么鸡蛋的食物在日常生活中会有很多重要问题。今天,\n",
      "Ep 3 (Step 102500): Train loss 2.639, Val loss 2.550\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。有个小朋友为家里的老师说:\"我都吸红钱,而我还是有一名好生命的人\",那么问题来了,他们究竟\n",
      "Ep 3 (Step 103000): Train loss 2.587, Val loss 2.561\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 点击到老年人群,我都有一定的答案,这是我认真学习的一个最好方法,如果我们想把每个人却总有一点进\n",
      "Ep 3 (Step 103500): Train loss 2.531, Val loss 2.535\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。可是对于大部分钱都不会耐心,我们还没想到,他们最后却遭受一些\"无知\"了,就是不幸懂事\n",
      "Ep 3 (Step 104000): Train loss 2.571, Val loss 2.541\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这一行星,不过是因为你在写给我很多文章后,还算有点不理解了。我觉得自己的钥匙,我们要赶紧就\n",
      "Ep 3 (Step 104500): Train loss 2.569, Val loss 2.542\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 在这个周天,我才知道是真正用了无痛患白血疫的食物之一了,并将病情推进到家庭\n",
      "Ep 3 (Step 105000): Train loss 2.446, Val loss 2.544\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。对于学校,大多数父母都在做过宝宝的时候自然会感觉厌望自己的孩子能够得到很\n",
      "Ep 3 (Step 105500): Train loss 2.528, Val loss 2.540\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这个家庭中有一个\"老大娃\",他是怎么回来的?我说自己一夜吃鸡蛋,但宝宝没有得�\n",
      "Ep 3 (Step 106000): Train loss 2.515, Val loss 2.549\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。在我们日常生活中,大多数人并不重视,什么是忘记养护的时间呢?就是我们这次来讨论。我们先了解一下如\n",
      "Ep 3 (Step 106500): Train loss 2.530, Val loss 2.548\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。大海一片碧波就有一段激荡。这古德语意为\"破纳钥匙\",它叫做\"青蛛�\n",
      "Ep 3 (Step 107000): Train loss 2.474, Val loss 2.542\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。想想,从这一点,先跟大家分享一下,快速脱贫的方法吧!快速脱贫治疗与治疗技\n",
      "Ep 3 (Step 107500): Train loss 2.646, Val loss 2.545\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。她在刚入学时,都有人要在快来一点小鼻炎之后走了。这不过,她还会因为没办法地给他们\n",
      "Ep 3 (Step 108000): Train loss 2.594, Val loss 2.558\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。德国总统布鲁娜·勒兹(roberti g stin chawles ken r l. k.s. k ng)在大漠堡留矛盾\n",
      "Ep 3 (Step 108500): Train loss 2.570, Val loss 2.557\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这是为什么呢? 我们知道: \"别看孩子,有什么可供爱?\" 以前都在小编的电影《给人一种真实和平\n",
      "Ep 3 (Step 109000): Train loss 2.546, Val loss 2.541\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这些话语中我就没有什么帮助,还是用不对你说,也没多少人说服了。那我一直以为就是\"旧事\",真不会\n",
      "Ep 3 (Step 109500): Train loss 2.535, Val loss 2.563\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 班主任老师指正: 一位家长在为孩子做了什么时才能不听话,不管是学校还是独立学校\n",
      "Ep 3 (Step 110000): Train loss 2.545, Val loss 2.540\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。一年后,原本不停的出血就已经开始了,其随意性感觉和意识都很强,可见在日常生活中大家还是可以通过\n",
      "Ep 3 (Step 110500): Train loss 2.501, Val loss 2.542\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 剪图 长宫 2010-08-08 记者 老西 王孚 潘永锺 本报记者 黄潭村 一、\n",
      "Ep 3 (Step 111000): Train loss 2.575, Val loss 2.558\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。有这么一段时间,在大海出现了一个\"忘记帝国灭亡\"的事件: 1920年的时候,我和媒体一起讨论�\n",
      "Ep 3 (Step 111500): Train loss 2.463, Val loss 2.552\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。这段时间,很多人都对宇宙并不了解了。然而,许多人则认为这样是大概的,就算一个也没有发生过病变\n",
      "Ep 3 (Step 112000): Train loss 2.513, Val loss 2.540\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。那时随着大家平安都在炮一边,而且还不用疯一反之。有人说它是指出门来去了,这些人就\n",
      "Ep 3 (Step 112500): Train loss 2.536, Val loss 2.554\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。当然还会在家里打开锅炼。我这就是这样一个问题。其实不仅要求我们能够解决问题,而且要做几个步\n",
      "Ep 3 (Step 113000): Train loss 2.439, Val loss 2.546\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。随着我想要迎接一步步走入家中,很多朋友都会出去玩耍,这样看似也是个小事情。而且\n",
      "Ep 3 (Step 113500): Train loss 2.474, Val loss 2.556\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 前期,我咱们把瓜子放在家里,或是把孔子放置于一些场合,当地的大街口的大坑袋�\n",
      "Ep 3 (Step 114000): Train loss 2.591, Val loss 2.535\n",
      "早上出门的时候我才发现忘记带钥匙，只好又回到家里。 但这时,我想从这个地域上来理解一下自己在家乡的区别和用途。 针对自己搞的牛顿\n",
      "Ep 3 (Step 114500): Train loss 2.478, Val loss 2.532\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      5\u001b[39m optimizer = torch.optim.AdamW( \n\u001b[32m      6\u001b[39m     model.parameters(), \n\u001b[32m      7\u001b[39m     lr=\u001b[32m0.0004\u001b[39m, \n\u001b[32m      8\u001b[39m     weight_decay=\u001b[32m0.1\u001b[39m \n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m num_epochs = \u001b[32m10\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m train_losses, val_losses, tokens_seen = \u001b[43mtrain_model_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43m     \u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_context\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m早上出门的时候我才发现忘记带钥匙，只好又回到家里。\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtrain_model_simple\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001b[39m\n\u001b[32m     12\u001b[39m optimizer.zero_grad()             \n\u001b[32m     13\u001b[39m loss = calc_loss_batch(\n\u001b[32m     14\u001b[39m     input_batch, target_batch, model, device \n\u001b[32m     15\u001b[39m )             \n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m optimizer.step()             \n\u001b[32m     18\u001b[39m tokens_seen += input_batch.numel() \n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/hjzd/miniforge3/envs/build_llm/lib/python3.11/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/hjzd/miniforge3/envs/build_llm/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/hjzd/miniforge3/envs/build_llm/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\") \n",
    "torch.manual_seed(123) \n",
    "model = GPTModel(GPT_CONFIG_124M) \n",
    "model.to(device) \n",
    "optimizer = torch.optim.AdamW( \n",
    "    model.parameters(), \n",
    "    lr=0.0004, \n",
    "    weight_decay=0.1 \n",
    ")\n",
    "num_epochs = 10 \n",
    "train_losses, val_losses, tokens_seen = train_model_simple(     \n",
    "    model, train_loader, val_loader, optimizer, device,     \n",
    "    num_epochs=num_epochs, eval_freq=500, eval_iter=10,     \n",
    "    start_context=\"早上出门的时候我才发现忘记带钥匙，只好又回到家里。\", tokenizer=tokenizer\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77b220f-b46c-4d11-bc10-49e0ccb643b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
