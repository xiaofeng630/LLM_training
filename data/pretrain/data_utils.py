import os
import numpy as np
from typing import Union
import json
import numpy as np
from pathlib import Path
from tqdm import tqdm
import tiktoken
import multiprocessing as mp

## ç”¨äºè®¡ç®—binæ–‡ä»¶çš„tokenæ•°é‡
def count_tokens_in_bin(
    path: Union[str, os.PathLike],
    dtype: str = "int32",
    suffix: str = ".bin",
    recursive: bool = True,
    ):
    """
    ç»Ÿè®¡ bin æ–‡ä»¶ä¸­çš„ token æ•°é‡

    å‚æ•°:
        path: å•ä¸ª .bin æ–‡ä»¶ æˆ– åŒ…å« .bin æ–‡ä»¶çš„ç›®å½•
        dtype: token çš„æ•°æ®ç±»å‹: "int32" | "int16" | "int64"
        suffix: bin æ–‡ä»¶åç¼€
        recursive: æ˜¯å¦é€’å½’ç»Ÿè®¡å­ç›®å½•

    è¿”å›:
        total_tokens (int)
    """
    dtype_map = {
        "int16": np.int16,
        "int32": np.int32,
        "int64": np.int64,
    }

    if dtype not in dtype_map:
        raise ValueError(f"Unsupported dtype: {dtype}")

    np_dtype = dtype_map[dtype]
    total_tokens = 0
    file_count = 0

    def count_file(file_path):
        nonlocal total_tokens, file_count
        data = np.fromfile(file_path, dtype=np_dtype)
        total_tokens += data.size
        file_count += 1

    if os.path.isfile(path):
        count_file(path)

    elif os.path.isdir(path):
        if recursive:
            for root, _, files in os.walk(path):
                for fname in files:
                    if fname.endswith(suffix):
                        count_file(os.path.join(root, fname))
        else:
            for fname in os.listdir(path):
                if fname.endswith(suffix):
                    count_file(os.path.join(path, fname))
    else:
        raise FileNotFoundError(path)

    print(f"Scanned {file_count} bin files")
    print(f"Total tokens: {total_tokens:,}")
    print(f"â‰ˆ {total_tokens / 1e9:.3f} B tokens")

    return total_tokens

def process_one_jsonl(args):
    jsonl_path, out_dir, enc_name, eos_token_id = args

    tokenizer = tiktoken.get_encoding(enc_name)

    out_path = out_dir / (jsonl_path.stem + ".bin")
    if out_path.exists():
        print(f"[skip] {out_path}")
        return

    print(f"[processing] {jsonl_path.name}")
    all_tokens = []

    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                text = json.loads(line).get("text", "")
                if not text:
                    continue
                tokens = tokenizer.encode(text)
                all_tokens.extend(tokens)
                all_tokens.append(eos_token_id)
            except Exception:
                continue

    arr = np.array(all_tokens, dtype=np.uint32) ## è¿™é‡Œå°¤å…¶éœ€è¦æ³¨æ„dtype, è¿™ä¸ªä¸ä¼šå½±å“æ¨¡å‹çš„æ•ˆæœ, æ˜¯æ ¹æ®tokenizerçš„vocab_sizeæ¥å®šçš„, å®šå°äº†ä¼šæœ‰æº¢å‡ºé£é™©
    arr.tofile(out_path)

    print(f"[saved] {out_path} | tokens={len(arr):,}")


def pretokenize_jsonl_dir_mp(jsonl_dir, out_dir, enc_name, eos_token_id, num_workers):
    jsonl_dir = Path(jsonl_dir)
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    jsonl_files = sorted(jsonl_dir.glob("*.jsonl"))

    tasks = [
        (path, out_dir, enc_name, eos_token_id)
        for path in jsonl_files
    ]

    with mp.Pool(processes=num_workers) as pool:
        pool.map(process_one_jsonl, tasks)

if __name__ == "__main__":
    total_tokens = count_tokens_in_bin(
        "/home/hjzd/lzz/LLM_training/data/pretrain/CCI3/data_bin/train",
        dtype="int32"
    )

    # enc_name = "cl100k_base"
    # tokenizer = tiktoken.get_encoding(enc_name)

    # pretokenize_jsonl_dir_mp(
    #     jsonl_dir="/home/hjzd/lzz/LLM_training/data/pretrain/CCI3/original_data/train",
    #     out_dir="/home/hjzd/lzz/LLM_training/data/pretrain/CCI3/data_bin/train",
    #     enc_name=enc_name,
    #     eos_token_id=tokenizer.eot_token,
    #     num_workers=4,  # ğŸ‘ˆ æ ¹æ® CPU æ ¸æ•°è°ƒ
    # )
